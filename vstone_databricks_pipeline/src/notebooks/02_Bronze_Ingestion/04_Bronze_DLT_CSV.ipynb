{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b377e716-05e2-4ee8-8471-dc9839edd801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e97bb56c-cb3b-4228-a757-231f7fd1015e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"vstone_project\"  # Matches the icon with the box\n",
    "schema_name = \"db_project\"      # Matches the icon with the database cylinders\n",
    "volume_path = \"/Volumes/vstone_project/db_project/raw_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ff53bb-fbaf-4145-8eff-0076248dffb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting last chunk of transactions data from chunk2_incremental location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c29d51cd-1f4d-4d4d-ae38-659bb2fb1365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, expr, lit, current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_transactions_incremental_dlt\",\n",
    "    comment=\"Individual Project: Incremental ingestion with unified date parsing and dynamic pathing\"\n",
    ")\n",
    "def bronze_transactions():\n",
    "    # 1. Retrieve the volume path from the Spark configuration\n",
    "    # This MUST be inside the function to avoid the 'None.get' Java error.\n",
    "    # The second argument is a fallback path for local testing.\n",
    "    v_path = spark.conf.get(\"pipeline.volume_path\", \"/Volumes/vstone_project/db_project/raw_data\")\n",
    "    \n",
    "    # 2. Define the source location\n",
    "    source_location = f\"{v_path}/chunks/chunk2_incremental\"\n",
    "    \n",
    "    # 3. Read the Stream using Autoloader (cloudFiles)\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(source_location)\n",
    "        .select(\n",
    "            # Primary Key\n",
    "            col(\"id\").cast(\"string\"),\n",
    "            \n",
    "            # Dimension Keys (Foreign Keys)\n",
    "            col(\"marka\").cast(\"string\"),\n",
    "            col(\"model\").cast(\"string\"),\n",
    "            col(\"currency\").cast(\"string\"),\n",
    "            col(\"place\").cast(\"string\"),\n",
    "            \n",
    "            # Unified Date Parsing Logic\n",
    "            expr(\"\"\"\n",
    "                coalesce(\n",
    "                    to_date(date, 'dd.MM.yyyy'),\n",
    "                    to_date(date, \"yyyy-MM-dd'T'HH:mm:ss'Z'\"),\n",
    "                    to_date(date, 'yyyy-MM-dd')\n",
    "                )\n",
    "            \"\"\").alias(\"date\"),\n",
    "            \n",
    "            # Measures and Casts\n",
    "            expr(\"try_cast(cost as double)\").alias(\"cost\"),\n",
    "            expr(\"try_cast(year as int)\").alias(\"year\"),\n",
    "            expr(\"try_cast(has_license as boolean)\").alias(\"has_license\"),\n",
    "            expr(\"try_cast(power as int)\").alias(\"power\"),\n",
    "            expr(\"try_cast(probeg as long)\").alias(\"probeg\"),\n",
    "            \n",
    "            # Metadata for Auditing\n",
    "            lit(\"chunk2_incremental\").alias(\"source_file\"),\n",
    "            current_timestamp().alias(\"load_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4570c4-45b0-4b3d-a215-2a3daad5b934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unifying bronze_transactions and bronze_transactions_incremental_dlt into one single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "716df14e-d38e-4bde-9286-13e6cc537476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"bronze_transactions_unified\",\n",
    "    comment=\"Union of manual ingestions and DLT incremental ingestion\"\n",
    ")\n",
    "def bronze_transactions_unified():\n",
    "    # Read the manually ingested table as a static source\n",
    "    df_manual = spark.read.table(f\"{catalog_name}.{schema_name}.bronze_transactions\")\n",
    "    \n",
    "    # Read the DLT-managed table\n",
    "    # Note: Use dlt.read() to reference other tables in the same pipeline\n",
    "    df_dlt = dlt.read(\"bronze_transactions_incremental_dlt\")\n",
    "    \n",
    "    # Return the union\n",
    "    return df_manual.unionByName(df_dlt, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7621b23-bb8c-4010-b0e6-a36f952d2ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting Photo csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9722c60d-0a81-453b-898f-b238c9291de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "photo_source_location = f\"{volume_path}/photo_data/\" # Update to your actual path\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_photos\",\n",
    "    comment=\"Ingestion of transaction photo URLs\"\n",
    ")\n",
    "def bronze_photos():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(photo_source_location)\n",
    "        .select(\n",
    "            # Removing the first column (index '0') and casting\n",
    "            col(\"id\").cast(\"string\"),\n",
    "            col(\"photo_url\").cast(\"string\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc86415-5916-4302-b59f-658c839a3b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89bb3b19-1604-4fa3-b415-f89a14cdabbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update this path to the actual location of your 1_text.csv\n",
    "text_source_location = f\"{volume_path}/text_data/\"\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_texts\",\n",
    "    comment=\"Ingesting car descriptions with long ID\"\n",
    ")\n",
    "def bronze_texts():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", \"true\") \n",
    "        .load(text_source_location)\n",
    "        .select(\n",
    "            # Convert \"47937696.0\" -> 47937696 (long)\n",
    "            expr(\"cast(cast(id as double) as long)\").alias(\"id\"),\n",
    "            col(\"text\").cast(\"string\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca64b544-d7db-4d36-ac0a-d58eafef060a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9c9c0b6-85a7-4678-96e8-b9379033d605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_source_location = f\"{volume_path}/geo_data/\"\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_geographic\",\n",
    "    comment=\"Geographic mapping for city names and coordinates\"\n",
    ")\n",
    "def bronze_geographic():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(geo_source_location)\n",
    "        .select(\n",
    "            col(\"name_padesh\").alias(\"city_name\"),\n",
    "            # This 'greate_padesh' is what matches your 'place' column in transactions\n",
    "            col(\"greate_padesh\").alias(\"city_prepositional\"), \n",
    "            col(\"lat\").cast(\"double\"),\n",
    "            col(\"lon\").cast(\"double\"),\n",
    "            lit(\"final_geografic.csv\").alias(\"source_file\"),\n",
    "            current_timestamp().alias(\"load_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f3c030-6733-471d-8e06-3008ea220df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting catalogs data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01af3632-67cb-49b7-9b43-68e0521ee3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_source_location = f\"{volume_path}/catalogs\"\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_catalogs\",\n",
    "    comment=\"Individual Project: Correcting Russian headers and semicolon delimiter\"\n",
    ")\n",
    "def bronze_catalogs():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"delimiter\", \";\")  # Mandatory for your specific file\n",
    "        .load(catalog_source_location)\n",
    "        .select(\n",
    "            # We MUST alias these Russian names to English to build the Star Schema later\n",
    "            col(\"Марка\").alias(\"marka\"),\n",
    "            col(\"Модель\").alias(\"model\"),\n",
    "            col(\"Поколение\").alias(\"generation\"),\n",
    "            col(\"Комплектация\").alias(\"version\"),\n",
    "            col(\"Тип кузова\").alias(\"body_type\"),\n",
    "            lit(\"catalogs.csv\").alias(\"source_file\"),\n",
    "            current_timestamp().alias(\"load_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70bafcb7-a782-4b47-a874-1548a81f64d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8887546165368554,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Bronze_DLT_CSV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
