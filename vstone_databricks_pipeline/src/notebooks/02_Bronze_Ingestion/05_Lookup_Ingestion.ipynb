{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69cc1d9-91d7-49d7-bbe1-5376c0bc2f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d465c959-1b1c-447f-937b-fd031eed764e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this in your Ingestion Notebook\n",
    "#%run \"../00_Setup/project_config\"\n",
    "import re\n",
    "\n",
    "cat_csv_path = f\"{volume_path}/catalogs.csv\"\n",
    "\n",
    "# 1. Read with the correct delimiter (sep=\";\")\n",
    "df_cat_raw = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"sep\", \";\") # This is the fix!\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(cat_csv_path))\n",
    "\n",
    "# 2. Sanitize column names to remove spaces and special characters\n",
    "def clean_column_name(name):\n",
    "    return re.sub(r'[ ,;{}()\\n\\t=]+', '_', name).strip('_')\n",
    "\n",
    "df_cat_clean = df_cat_raw.toDF(*[clean_column_name(c) for c in df_cat_raw.columns])\n",
    "\n",
    "# 3. Overwrite the broken table\n",
    "df_cat_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.catalogs_bronze\")\n",
    "\n",
    "print(\"✅ Catalogs table fixed! New Columns:\")\n",
    "print(df_cat_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e89d8c-a6d9-4a00-84a2-6aa6e44a2022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this in your 02_Bronze_Ingestion folder\n",
    "#%run \"../00_Setup/project_config\"\n",
    "import re\n",
    "\n",
    "cat_csv_path = f\"{volume_path}/catalogs.csv\"\n",
    "\n",
    "# 1. Read with the correct semicolon separator\n",
    "df_cat_raw = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"sep\", \";\") # The critical fix\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(cat_csv_path))\n",
    "\n",
    "# 2. Sanitize column names (removes spaces/dots for Delta compatibility)\n",
    "def clean_col(name):\n",
    "    return re.sub(r'[ ,;{}()\\n\\t=]+', '_', name).strip('_')\n",
    "\n",
    "df_cat_clean = df_cat_raw.toDF(*[clean_col(c) for c in df_cat_raw.columns])\n",
    "\n",
    "# 3. Overwrite the catalog table\n",
    "df_cat_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.catalogs_bronze\")\n",
    "\n",
    "print(\"✅ Catalogs table fixed! Individual columns are now available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733da6c7-4e13-488d-af7c-0698a712d1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the existing broken table to clear metadata\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.catalogs_bronze\")\n",
    "\n",
    "# Re-ingest properly\n",
    "df_cat_fixed = (spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"sep\", \";\") \n",
    "                .load(f\"{volume_path}/catalogs.csv\"))\n",
    "\n",
    "# Sanitize and Save\n",
    "import re\n",
    "clean_cols = [re.sub(r'[ ,;{}()\\n\\t=]+', '_', c).strip('_') for c in df_cat_fixed.columns]\n",
    "df_cat_fixed.toDF(*clean_cols).write.format(\"delta\").saveAsTable(f\"{catalog_name}.{schema_name}.catalogs_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1ab8b20-2741-43ef-8d9f-c7d7025b55dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Lookup_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
