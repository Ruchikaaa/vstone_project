{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06266b92-1bb0-4a8c-9f53-16ed1f1b5509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a015c8-566c-4a71-a1e4-ccbee988ebd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp, col, expr\n",
    "\n",
    "# 2. Re-ingest with multiline support\n",
    "csv_path = f\"{volume_path}/chunks/chunk1_initial\"\n",
    "df_csv = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\") \n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .load(csv_path))\n",
    "\n",
    "# 3. Robust Date and Type Transformation\n",
    "df_csv_cleaned = df_csv.select(\n",
    "    col(\"id\").cast(\"string\"),\n",
    "    col(\"marka\").cast(\"string\"),\n",
    "    col(\"model\").cast(\"string\"),\n",
    "    expr(\"try_cast(year as int)\").alias(\"year\"),\n",
    "    expr(\"try_cast(cost as double)\").alias(\"cost\"),\n",
    "    col(\"currency\").cast(\"string\"),\n",
    "    expr(\"try_cast(has_license as boolean)\").alias(\"has_license\"),\n",
    "    col(\"place\").cast(\"string\"),\n",
    "    # THE FIX: Try ISO format first, then European dots\n",
    "    expr(\"\"\"\n",
    "        coalesce(\n",
    "            try_to_date(date, \"yyyy-MM-dd'T'HH:mm:ss'Z'\"),\n",
    "            try_to_date(date, 'dd.MM.yyyy'),\n",
    "            try_to_date(date, 'yyyy-MM-dd')\n",
    "        )\n",
    "    \"\"\").alias(\"date\"),\n",
    "    col(\"engine\").cast(\"string\"),\n",
    "    lit(\"chunk1_initial.csv\").alias(\"source_file\"),\n",
    "    current_timestamp().alias(\"load_timestamp\")\n",
    ")\n",
    "\n",
    "# 4. Append to Bronze\n",
    "target_table = f\"{catalog_name}.{schema_name}.bronze_transactions\"\n",
    "(df_csv_cleaned.write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(target_table))\n",
    "\n",
    "print(f\"‚úÖ Success! Re-ingested {df_csv_cleaned.count()} rows with mixed date formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ade841-ad6f-4406-b79b-5218aa7d45e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 2. Use the 'bronze_main' variable which holds 'vstone_project.db_project.bronze_transactions'\n",
    "try:\n",
    "    sources = spark.table(bronze_main).select(\"source_file\").distinct().collect()\n",
    "    source_list = [row['source_file'] for row in sources]\n",
    "\n",
    "    expected_sources = [\"chunk1_initial.csv\", \"chunk3_json\", \"chunk4_xml\"]\n",
    "\n",
    "    # 3. Check if every expected source is in our table\n",
    "    for expected in expected_sources:\n",
    "        assert expected in source_list, f\"Missing Source Check Failed: {expected} not found!\"\n",
    "\n",
    "    print(f\"‚úÖ Format Check Passed. Sources found: {source_list}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97609097-1ab9-4156-9b31-27d35f0f25d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# 2. Define critical columns\n",
    "critical_columns = [\"id\", \"cost\", \"marka\", \"model\"]\n",
    "\n",
    "# 3. Perform a single pass aggregation for efficiency\n",
    "print(f\"üîç Analyzing null percentages in {bronze_main}...\")\n",
    "\n",
    "# Create dynamic expressions to count nulls for each critical column\n",
    "null_exprs = [count(when(col(c).isNull(), c)).alias(c) for c in critical_columns]\n",
    "total_count = spark.table(bronze_main).count()\n",
    "\n",
    "# Execute the null count\n",
    "null_counts_row = spark.table(bronze_main).select(*null_exprs).collect()[0]\n",
    "\n",
    "# 4. Verify results\n",
    "for column in critical_columns:\n",
    "    null_count = null_counts_row[column]\n",
    "    null_percentage = (null_count / total_count) * 100\n",
    "    \n",
    "    # Assert check\n",
    "    assert null_percentage < 90, f\"‚ùå Schema Check Failed: Column '{column}' is {null_percentage:.2f}% null!\"\n",
    "    print(f\"   ‚úÖ Column '{column}': {null_percentage:.2f}% null (Total nulls: {null_count})\")\n",
    "\n",
    "print(f\"\\n‚ú® Data Quality Check Passed for {total_count} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745c5269-1573-400e-829a-0f8e59e41d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED vstone_project.db_project.bronze_transactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0fc3dd7-5f00-4e25-863f-c39250e8b739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8887546165368566,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Bronze_MultiFormat_Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
