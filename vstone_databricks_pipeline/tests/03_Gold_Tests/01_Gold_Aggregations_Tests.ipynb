{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582a31cc-4e43-4af7-8640-b1547cbd5073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac61d4ca-13b7-4bbe-9a8f-b0cf100aa401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, lit, current_timestamp, count, avg, round, sum, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f57c612-37da-48bb-af05-07d179955ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Star Schema PK/FK Join Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da4d322-9cc9-40fa-804e-b4f4564ab6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_gold_star_schema_join():\n",
    "    # Setup mock data for Fact and Dim\n",
    "    sales_data = [(\"TXN101\", \"CITY_01\", \"Toyota\", \"Camry\")]\n",
    "    catalog_data = [(\"Toyota\", \"Camry\", \"Gen8\", \"Sedan\")]\n",
    "    \n",
    "    df_sales = spark.createDataFrame(sales_data, [\"transaction_id\", \"geo_id\", \"marka\", \"model\"])\n",
    "    df_cat = spark.createDataFrame(catalog_data, [\"marka\", \"model\", \"generation\", \"body_type\"])\n",
    "    \n",
    "    # Test the join logic used in Gold\n",
    "    result_df = df_sales.join(df_cat, [\"marka\", \"model\"], \"left\")\n",
    "    \n",
    "    # Verify attributes from Dim are present in the result\n",
    "    row = result_df.collect()[0]\n",
    "    assert row[\"body_type\"] == \"Sedan\", \"FK Join to Dim_Catalogs failed\"\n",
    "    assert \"generation\" in result_df.columns, \"Dimension attributes missing from Fact table\"\n",
    "    print(\"Test 1 Passed: Star Schema PK/FK joins are working correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5739747b-ef78-41ea-b204-314681e06105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Geographical KPI Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b421bfc9-e446-4961-a7bb-56533e24fa30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, count\n",
    "\n",
    "def test_gold_geo_kpi_math():\n",
    "    # Setup mock sales for a single city\n",
    "    data = [\n",
    "        (\"TXN1\", \"Moscow\", 10000.0),\n",
    "        (\"TXN2\", \"Moscow\", 20000.0)\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"transaction_id\", \"city_name\", \"cost\"])\n",
    "    \n",
    "    # Test aggregation logic\n",
    "    kpi_df = df.groupBy(\"city_name\").agg(\n",
    "        count(\"transaction_id\").alias(\"listing_count\"),\n",
    "        avg(\"cost\").alias(\"avg_listing_price\"),\n",
    "        sum(\"cost\").alias(\"total_market_value\")\n",
    "    )\n",
    "    \n",
    "    res = kpi_df.collect()[0]\n",
    "    assert res[\"listing_count\"] == 2, \"Listing count calculation failed\"\n",
    "    assert res[\"avg_listing_price\"] == 15000.0, \"Average price calculation failed\"\n",
    "    assert res[\"total_market_value\"] == 30000.0, \"Total market value calculation failed\"\n",
    "    print(\"Test 2 Passed: Geographical KPI calculations are accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eac672e4-b91e-41e9-a451-13d652af7410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Visual Content Impact Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f309a225-9484-4f4c-a305-002c44b3abab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def test_content_impact_ranges():\n",
    "    # Setup data with different photo counts\n",
    "    data = [(0,), (3,), (10,)]\n",
    "    df = spark.createDataFrame(data, [\"photo_count\"])\n",
    "    \n",
    "    # Apply your Gold Layer logic\n",
    "    result_df = df.withColumn(\"photo_range\", \n",
    "        expr(\"CASE WHEN photo_count = 0 THEN 'No Photos' WHEN photo_count < 5 THEN 'Low' ELSE 'High' END\"))\n",
    "    \n",
    "    results = {row[\"photo_count\"]: row[\"photo_range\"] for row in result_df.collect()}\n",
    "    assert results[0] == \"No Photos\", \"Zero photo range logic failed\"\n",
    "    assert results[3] == \"Low\", \"Low photo range logic failed\"\n",
    "    assert results[10] == \"High\", \"High photo range logic failed\"\n",
    "    print(\"Test 3 Passed: Visual Content impact ranges are correctly classified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7780d09-097d-4b95-80c3-b9fc5e2baac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Full Dataset Integration (Mandate 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfd3133-0b84-49f3-a847-bc8204941109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_gold_dataset_completeness():\n",
    "    # FIX: Use spark.table() instead of dlt.read() for interactive cell testing\n",
    "    # Ensure you use your actual catalog and schema names\n",
    "    gold_table_path = \"vstone_project.db_project.fact_car_sales\"\n",
    "    \n",
    "    try:\n",
    "        actual_cols = spark.table(gold_table_path).columns\n",
    "        \n",
    "        # Define required columns from each of the 5 files [Mandate 2]\n",
    "        required = {\n",
    "            \"cost\": \"From 1_main.csv\",\n",
    "            \"generation\": \"From catalogs.csv\",\n",
    "            \"city_name\": \"From final_geographic.csv\",\n",
    "            \"photo_count\": \"From 1_photo.csv\",\n",
    "            \"description\": \"From 1_text.csv\"\n",
    "        }\n",
    "        \n",
    "        for col_name, source in required.items():\n",
    "            assert col_name in actual_cols, f\"Missing data from {source} in Gold Layer\"\n",
    "            \n",
    "        print(\"Test 4 Passed: All 5 dataset files are successfully integrated into the Gold Fact table.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test 4 Failed: Could not read Gold table. Error: {e}\")\n",
    "\n",
    "# Run the final test\n",
    "test_gold_dataset_completeness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e529019-a84e-433e-adbf-53ca4a00711d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_gold_star_schema_join()\n",
    "test_gold_geo_kpi_math()\n",
    "test_content_impact_ranges()\n",
    "test_gold_dataset_completeness()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Gold_Aggregations_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
