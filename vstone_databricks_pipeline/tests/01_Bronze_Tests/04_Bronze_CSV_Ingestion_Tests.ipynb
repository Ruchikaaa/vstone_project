{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727ba366-265a-42f5-ae17-b8c11704dcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a55813f-4b71-4351-bfe1-78728730be18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def test_bronze_logic():\n",
    "    # 1. Create mock data for Text (testing double -> long cast) and Transactions (testing date)\n",
    "    mock_text_data = spark.createDataFrame([Row(id=\"47937696.0\", text=\"Great car\")])\n",
    "    mock_trans_data = spark.createDataFrame([Row(date=\"15.01.2023\", year=\"2022\", id=\"1\")])\n",
    "\n",
    "    # 2. Test Text ID Logic\n",
    "    res_text = mock_text_data.select(F.expr(\"cast(cast(id as double) as long)\").alias(\"id\"))\n",
    "    assert res_text.first()[\"id\"] == 47937696, \"Text ID casting failed!\"\n",
    "\n",
    "    # 3. Test Transaction Date/Year Logic\n",
    "    res_trans = mock_trans_data.select(\n",
    "        F.expr(\"try_cast(year as int)\").alias(\"year\"),\n",
    "        F.expr(\"\"\"\n",
    "            coalesce(\n",
    "                to_date(date, 'dd.MM.yyyy'),\n",
    "                to_date(date, \"yyyy-MM-dd'T'HH:mm:ss'Z'\"),\n",
    "                to_date(date, 'yyyy-MM-dd')\n",
    "            )\n",
    "        \"\"\").alias(\"date\")\n",
    "    )\n",
    "    \n",
    "    row = res_trans.first()\n",
    "    assert row[\"year\"] == 2022\n",
    "    assert row[\"date\"].strftime(\"%Y-%m-%d\") == \"2023-01-15\", \"Date Coalesce logic failed!\"\n",
    "    \n",
    "    print(\"✅ All Logic Unit Tests Passed!\")\n",
    "\n",
    "test_bronze_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c190f1b0-463a-4511-87c9-6d4e7e2801a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verify_dlt_deployment():\n",
    "    # 1. Verify Unified Table Count\n",
    "    unified_df = spark.table(\"LIVE.bronze_transactions_unified\")\n",
    "    count = unified_df.count()\n",
    "    assert count > 0, \"Unified table is empty!\"\n",
    "\n",
    "    # 2. Verify Geo-Mapping completeness\n",
    "    # Ensure 'city_prepositional' exists so it can join with 'place' later\n",
    "    geo_df = spark.table(\"LIVE.bronze_geographic\")\n",
    "    null_geo = geo_df.filter(F.col(\"city_prepositional\").isNull()).count()\n",
    "    assert null_geo == 0, f\"Found {null_geo} rows in Geographic table with missing join keys!\"\n",
    "\n",
    "    # 3. Verify Photo URLs\n",
    "    photos_df = spark.table(\"LIVE.bronze_photos\")\n",
    "    assert \"photo_url\" in photos_df.columns, \"Photo URL column missing!\"\n",
    "    \n",
    "    print(f\"✅ Deployment Verified: {count} unified transactions ready for Silver layer.\")\n",
    "\n",
    "# Note: 'LIVE' is the default schema name inside a DLT pipeline context. \n",
    "# Outside DLT, use your catalog.schema name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c9afbd6-ee02-453b-b32a-0dab2c6e316c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Bronze_CSV_Ingestion_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
