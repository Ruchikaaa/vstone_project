{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580c3568-88b0-47d8-90f8-d3ca7217c975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47f9a76-80a4-48cd-8f30-72cea6a18840",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Setup Mock Data\n",
    "data = [\n",
    "    (\"1\", \"Toyota\", \"2022\", \"15.01.2023\"),           # dd.MM.yyyy\n",
    "    (\"2\", \"Honda\", \"2021\", \"2023-05-20T10:00:00Z\"),  # ISO Timestamp\n",
    "    (\"3\", \"Ford\", \"UNKNOWN\", \"2023-12-01\")           # yyyy-MM-dd\n",
    "]\n",
    "columns = [\"id\", \"marka\", \"year\", \"date\"]\n",
    "mock_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 2. Apply Transformation\n",
    "test_df = mock_df.select(\n",
    "    F.col(\"id\").cast(\"string\"),\n",
    "    # FIX: Use F.expr to call try_cast. This bypasses the ANSI mode crash.\n",
    "    F.expr(\"try_cast(year as int)\").alias(\"year\"), \n",
    "    F.coalesce(\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit('dd.MM.yyyy')),\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")),\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit('yyyy-MM-dd'))\n",
    "    ).cast(\"date\").alias(\"date_parsed\")\n",
    ")\n",
    "\n",
    "# 3. Assertions\n",
    "results = test_df.collect()\n",
    "\n",
    "# Test Row 1: dd.MM.yyyy\n",
    "assert results[0][\"date_parsed\"].strftime('%Y-%m-%d') == \"2023-01-15\", \"Failed: dd.MM.yyyy\"\n",
    "\n",
    "# Test Row 2: ISO Timestamp\n",
    "assert results[1][\"date_parsed\"].strftime('%Y-%m-%d') == \"2023-05-20\", \"Failed: ISO T-format\"\n",
    "\n",
    "# Test Row 3: Malformed Year\n",
    "# try_cast will return None instead of throwing [CAST_INVALID_INPUT]\n",
    "assert results[2][\"year\"] is None, \"Failed: Malformed year cast\"\n",
    "\n",
    "print(\"âœ… Logic Test Passed: Pipeline is now ANSI-safe and crash-proof!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5cd824-a8b6-45d9-bac4-02e0d17f710e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, DoubleType\n",
    "# Configuration\n",
    "table_name = \"vstone_project.db_project.bronze_transactions\"\n",
    "\n",
    "# 1. Assert Table Exists\n",
    "assert spark.catalog.tableExists(table_name), f\"Table {table_name} does not exist!\"\n",
    "\n",
    "# 2. Assert Data Ingested (Count > 0)\n",
    "row_count = spark.table(table_name).count()\n",
    "assert row_count > 0, f\"Failure: {table_name} is empty after ingestion!\"\n",
    "\n",
    "# 3. Assert Schema Integrity\n",
    "schema = spark.table(table_name).schema\n",
    "# Check that 'date' is specifically a DateType and not a StringType\n",
    "assert isinstance(schema[\"date\"].dataType, DateType), \\\n",
    "    f\"Schema Error: 'date' is {type(schema['date'].dataType)}, expected DateType\"\n",
    "assert isinstance(schema[\"cost\"].dataType, DoubleType), \\\n",
    "    f\"Schema Error: 'cost' is {type(schema['cost'].dataType)}, expected DoubleType\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18bef64a-650f-450d-b41e-304cc8cc6196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_Transactions_Ingestion_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
