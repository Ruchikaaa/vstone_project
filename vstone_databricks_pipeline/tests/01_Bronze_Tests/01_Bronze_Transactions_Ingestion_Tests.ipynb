{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580c3568-88b0-47d8-90f8-d3ca7217c975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3957c4d0-5dbd-4c04-ae27-e514155e18ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logic Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47f9a76-80a4-48cd-8f30-72cea6a18840",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Setup Mock Data\n",
    "data = [\n",
    "    (\"1\", \"Toyota\", \"2022\", \"15.01.2023\"),           # dd.MM.yyyy\n",
    "    (\"2\", \"Honda\", \"2021\", \"2023-05-20T10:00:00Z\"),  # ISO Timestamp\n",
    "    (\"3\", \"Ford\", \"UNKNOWN\", \"2023-12-01\")           # yyyy-MM-dd\n",
    "]\n",
    "columns = [\"id\", \"marka\", \"year\", \"date\"]\n",
    "mock_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 2. Apply Transformation\n",
    "test_df = mock_df.select(\n",
    "    F.col(\"id\").cast(\"string\"),\n",
    "    # FIX: Use F.expr to call try_cast. This bypasses the ANSI mode crash.\n",
    "    F.expr(\"try_cast(year as int)\").alias(\"year\"), \n",
    "    F.coalesce(\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit('dd.MM.yyyy')),\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")),\n",
    "        F.try_to_timestamp(F.col(\"date\"), F.lit('yyyy-MM-dd'))\n",
    "    ).cast(\"date\").alias(\"date_parsed\")\n",
    ")\n",
    "\n",
    "# 3. Assertions\n",
    "results = test_df.collect()\n",
    "\n",
    "# Test Row 1: dd.MM.yyyy\n",
    "assert results[0][\"date_parsed\"].strftime('%Y-%m-%d') == \"2023-01-15\", \"Failed: dd.MM.yyyy\"\n",
    "\n",
    "# Test Row 2: ISO Timestamp\n",
    "assert results[1][\"date_parsed\"].strftime('%Y-%m-%d') == \"2023-05-20\", \"Failed: ISO T-format\"\n",
    "\n",
    "# Test Row 3: Malformed Year\n",
    "# try_cast will return None instead of throwing [CAST_INVALID_INPUT]\n",
    "assert results[2][\"year\"] is None, \"Failed: Malformed year cast\"\n",
    "\n",
    "print(\"Logic Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c24239b-50b0-430e-b7a1-a959ba48b24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Integration Tests (Bronze Layer Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bef64a-650f-450d-b41e-304cc8cc6196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, DoubleType\n",
    "\n",
    "# Configuration\n",
    "target_table = \"vstone_project.db_project.bronze_transactions\"\n",
    "\n",
    "print(f\"Starting Integration Tests for {target_table}...\")\n",
    "\n",
    "# 1. Verification: Does the table exist in Unity Catalog?\n",
    "assert spark.catalog.tableExists(target_table), f\"Critical Error: Table {target_table} not found!\"\n",
    "\n",
    "# 2. Verification: Did data actually ingest?\n",
    "row_count = spark.table(target_table).count()\n",
    "assert row_count > 0, f\"Critical Error: {target_table} is empty!\"\n",
    "print(f\"Verified: {row_count} records found in Bronze layer.\")\n",
    "\n",
    "# 3. Verification: Schema Integrity (Data Types)\n",
    "# Ensuring 'date' is a proper Date object and 'cost' is numeric\n",
    "schema = spark.table(target_table).schema\n",
    "\n",
    "assert isinstance(schema[\"date\"].dataType, DateType), \\\n",
    "    f\"Type Mismatch: 'date' is {schema['date'].dataType}, expected DateType\"\n",
    "\n",
    "assert isinstance(schema[\"cost\"].dataType, DoubleType), \\\n",
    "    f\"Type Mismatch: 'cost' is {schema['cost'].dataType}, expected DoubleType\"\n",
    "\n",
    "print(\"Integration Tests Passed: Bronze table exists, contains data, and has correct data types.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_Transactions_Ingestion_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
