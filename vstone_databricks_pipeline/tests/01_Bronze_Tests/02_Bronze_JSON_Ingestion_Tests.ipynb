{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a829002b-7731-4785-aeaa-1c60dcba5859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806007aa-e9c2-46f1-bced-19c78feebc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logic Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c253baf-0ac4-4488-9ba5-68de306d73d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "# 1. Simulate JSON data using a simple list of Row objects or a collection\n",
    "# This avoids spark.sparkContext.parallelize\n",
    "json_data = [\n",
    "    {\"id\": \"A1\", \"year\": \"2024\", \"cost\": \"1500.50\", \"has_license\": \"true\", \"date\": \"2024-01-01\"},\n",
    "    {\"id\": \"A2\", \"year\": \"2023\", \"cost\": \"2000\", \"has_license\": \"false\", \"date\": \"2023-12-31\"}\n",
    "]\n",
    "\n",
    "# Create the DataFrame directly from the list of dictionaries\n",
    "raw_df = spark.createDataFrame(json_data)\n",
    "\n",
    "# 2. Apply your transformation logic\n",
    "# (Using F.expr for try_cast as we discussed earlier for ANSI safety)\n",
    "processed_df = raw_df.select(\n",
    "    F.col(\"id\").cast(\"string\"),\n",
    "    F.expr(\"try_cast(year as int)\").alias(\"year\"),\n",
    "    F.expr(\"try_cast(cost as double)\").alias(\"cost\"),\n",
    "    F.expr(\"try_cast(has_license as boolean)\").alias(\"has_license\"),\n",
    "    F.col(\"date\").cast(\"date\"),\n",
    "    F.lit(\"chunk3_json\").alias(\"source_file\"),\n",
    "    F.current_timestamp().alias(\"load_timestamp\")\n",
    ")\n",
    "\n",
    "# 3. Assertions\n",
    "sample = processed_df.collect()\n",
    "\n",
    "# Note: In Python, Spark's DoubleType maps to 'float' \n",
    "# and IntegerType maps to 'int'.\n",
    "assert isinstance(sample[0][\"year\"], int), \"Year should be converted to Integer\"\n",
    "assert isinstance(sample[0][\"cost\"], float), \"Cost should be converted to Double/Float\"\n",
    "assert sample[0][\"has_license\"] is True, \"has_license should be Boolean True\"\n",
    "assert sample[0][\"source_file\"] == \"chunk3_json\", \"Source file metadata missing\"\n",
    "\n",
    "print(\"✅ JSON Logic Test Passed: Serverless compatible and schema validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04d79b78-43d5-43fa-9ef7-fa3daea12a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Integration & Ingestion Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b79eec-4d7f-458a-902b-32b4595f9458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_full_name = f\"{catalog_name}.{schema_name}.bronze_transactions\"\n",
    "\n",
    "# 1. Wait for stream to finish (Safety check)\n",
    "# In a real notebook, ensure the streaming cell has finished running first.\n",
    "\n",
    "# 2. Assert data exists from this specific source\n",
    "json_ingest_count = spark.table(table_full_name) \\\n",
    "    .filter(F.col(\"source_file\") == \"chunk3_json\") \\\n",
    "    .count()\n",
    "\n",
    "if json_ingest_count == 0:    print(f\"Warning: No rows found for source 'chunk3_json' in {table_full_name}\")\n",
    "else:\n",
    "    assert json_ingest_count > 0, f\"Failure: No rows found for source 'chunk3_json' in {table_full_name}\"\n",
    "\n",
    "\n",
    "# 3. Assert no schema corruption\n",
    "# Auto Loader sometimes puts failed parses into _rescued_data if configured.\n",
    "# We check if 'has_license' or 'year' contain unexpected NULLs that suggest a hint failure.\n",
    "null_check = spark.table(table_full_name) \\\n",
    "    .filter(F.col(\"source_file\") == \"chunk3_json\") \\\n",
    "    .filter(F.col(\"year\").isNull() | F.col(\"has_license\").isNull()) \\\n",
    "    .count()\n",
    "\n",
    "assert null_check == 0, f\"Data Quality Error: {null_check} rows have NULL values in hinted columns!\"\n",
    "\n",
    "print(f\"✅ Ingestion Verified: {json_ingest_count} records successfully merged into Bronze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c32dd41-3661-4628-ac7d-45b27f901306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Bronze_JSON_Ingestion_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
