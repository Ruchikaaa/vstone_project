{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499b7bd5-769f-45db-90e9-8e470fddb46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa3f86f-936b-4913-b74d-c9ac0f900c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "def test_dimensions():\n",
    "    # TEST: Geography Uniqueness\n",
    "    # Ensure geo_id (the join key) is unique\n",
    "    geo_df = spark.table(f\"{catalog_name}.{schema_name}.dim_geography\")\n",
    "    geo_dupes = geo_df.groupBy(\"geo_id\").count().filter(\"count > 1\").count()\n",
    "    assert geo_dupes == 0, f\"Critical: Found {geo_dupes} duplicate geo_ids in dim_geography!\"\n",
    "\n",
    "    # TEST: Car Content Coverage\n",
    "    # Check if photo_count is correctly calculated (should be >= 0)\n",
    "    content_df = spark.table(f\"{catalog_name}.{schema_name}.dim_car_content\")\n",
    "    invalid_photos = content_df.filter(\"photo_count < 0\").count()\n",
    "    assert invalid_photos == 0, \"Logic Error: photo_count cannot be negative.\"\n",
    "\n",
    "    # TEST: Technical Specs Logic\n",
    "    # Ensure distinct worked and we don't have exact row duplicates\n",
    "    specs_df = spark.table(f\"{catalog_name}.{schema_name}.dim_technical_specs\")\n",
    "    assert specs_df.count() == specs_df.distinct().count(), \"Duplicate specs found in dim_technical_specs.\"\n",
    "\n",
    "    print(\"✅ Dimension Tests Passed: Uniqueness and logic verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ae644e-0807-4f52-a6cd-d4828de575fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_fact_and_quarantine():\n",
    "    bronze_count = spark.table(f\"{catalog_name}.{schema_name}.bronze_transactions_unified\").count()\n",
    "    fact_count = spark.table(f\"{catalog_name}.{schema_name}.fact_transactions\").count()\n",
    "    quarantine_count = spark.table(f\"{catalog_name}.{schema_name}.quarantine_transactions\").count()\n",
    "\n",
    "    # 1. Total Conservation Test\n",
    "    # Fact + Quarantine should equal Bronze (unless you have a filter in both)\n",
    "    assert bronze_count == (fact_count + quarantine_count), \\\n",
    "        f\"Data Loss: {bronze_count} rows in Bronze, but only {fact_count + quarantine_count} accounted for.\"\n",
    "\n",
    "    # 2. Fact Integrity: Price Check\n",
    "    # Ensure no rows in Fact violate the business rule (> 1000.0)\n",
    "    price_violations = spark.table(f\"{catalog_name}.{schema_name}.fact_transactions\").filter(\"cost <= 1000.0\").count()\n",
    "    assert price_violations == 0, \"Business Rule Violation: Low cost items found in fact_transactions!\"\n",
    "\n",
    "    # 3. Quarantine Accuracy: Reason Check\n",
    "    # Ensure all quarantine rows actually have a reason assigned\n",
    "    missing_reason = spark.table(f\"{catalog_name}.{schema_name}.quarantine_transactions\").filter(\"quarantine_reason IS NULL\").count()\n",
    "    assert missing_reason == 0, \"Logic Error: Rows quarantined without a reason.\"\n",
    "\n",
    "    print(f\"✅ Fact/Quarantine Tests Passed: {fact_count} cleaned, {quarantine_count} quarantined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4869907e-86a2-4fba-a2b7-7cb046b31a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_dimensions()\n",
    "test_fact_and_quarantine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83847ca-1859-451c-9efe-c84c242ee074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Silver_Transformations_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
