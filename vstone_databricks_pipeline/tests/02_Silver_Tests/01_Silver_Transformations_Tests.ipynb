{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499b7bd5-769f-45db-90e9-8e470fddb46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/ruchika.b.mhetre@v4c.ai/vstone_project/vstone_databricks_pipeline/src/notebooks/00_Setup/project_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816fb13e-0843-4e40-94d2-725cc7a7f64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Transaction Quarantine Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83847ca-1859-451c-9efe-c84c242ee074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_transaction_quarantine():\n",
    "    # Setup: Create a mock dataframe with one valid and one invalid record\n",
    "    data = [\n",
    "        (\"1\", \"CityA\", \"Toyota\", \"Camry\", 2020, 15000.0), # Valid\n",
    "        (None, \"CityB\", \"Honda\", \"Civic\", 2019, 500.0)    # Invalid: Missing ID & Cost < 1000\n",
    "    ]\n",
    "    columns = [\"id\", \"place\", \"marka\", \"model\", \"year\", \"cost\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    # Test logic for Silver Transactions\n",
    "    valid_df = df.filter(\"(id IS NOT NULL) AND (cost > 1000.0)\")\n",
    "    # Test logic for Quarantine\n",
    "    quarantine_df = df.filter(\"NOT ((id IS NOT NULL) AND (cost > 1000.0))\")\n",
    "    \n",
    "    assert valid_df.count() == 1, \"Should have 1 valid record\"\n",
    "    assert quarantine_df.count() == 1, \"Should have 1 quarantined record\"\n",
    "    print(\"Test 1 Passed: Quarantine logic is accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2281289-a0f1-4396-a6dd-409dbacc7a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Geography Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f0ccb21-1e47-4814-a7d7-6a8938f41a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_geography_deduplication():\n",
    "    # Setup: Duplicate entries for the same prepositional ID\n",
    "    data = [\n",
    "        (\"в Москве\", \"Moscow\", 55.7, 37.6),\n",
    "        (\"в Москве\", \"Moscow\", 55.7, 37.6)\n",
    "    ]\n",
    "    columns = [\"city_prepositional\", \"city_name\", \"lat\", \"lon\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    # Test deduplication\n",
    "    silver_geo_test = df.dropDuplicates([\"city_prepositional\"])\n",
    "    \n",
    "    assert silver_geo_test.count() == 1, \"Geography must be unique per city_prepositional\"\n",
    "    print(\"Test 2 Passed: Geography dimension is deduplicated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d5d2b9-80e5-4c66-82bb-473edac39de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Catalog Key Standardization (Trimming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42760d79-acea-460f-bb1e-ae363c8b938c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary Spark functions to prevent NameErrors\n",
    "from pyspark.sql.functions import col, trim, count\n",
    "\n",
    "def test_catalog_key_standardization():\n",
    "    # Setup: Data with inconsistent spacing\n",
    "    data = [(\" Toyota \", \" Camry \")]\n",
    "    columns = [\"marka\", \"model\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    # Test trimming - 'col' is now defined via the import above\n",
    "    silver_catalog_test = df.select(\n",
    "        trim(col(\"marka\")).alias(\"marka\"), \n",
    "        trim(col(\"model\")).alias(\"model\")\n",
    "    )\n",
    "    \n",
    "    row = silver_catalog_test.collect()[0]\n",
    "    assert row[\"marka\"] == \"Toyota\", \"Marka should be trimmed\"\n",
    "    assert row[\"model\"] == \"Camry\", \"Model should be trimmed\"\n",
    "    print(\"Test 3 Passed: Catalog join keys are standardized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd7a3e0-d5aa-420b-a34f-9e2ac2834abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test: Car Content Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9daa468d-3494-44ea-b924-0851ba74e281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_car_content_enrichment():\n",
    "    # Setup\n",
    "    text_data = [(\"101\", \"Excellent car\")]\n",
    "    photo_data = [(\"101\", \"url1\"), (\"101\", \"url2\")]\n",
    "    \n",
    "    df_text = spark.createDataFrame(text_data, [\"id\", \"text\"])\n",
    "    df_photos = spark.createDataFrame(photo_data, [\"id\", \"photo_url\"])\n",
    "    \n",
    "    # Execution\n",
    "    photo_agg = df_photos.groupBy(\"id\").agg(count(\"photo_url\").alias(\"photo_count\"))\n",
    "    result_df = df_text.join(photo_agg, on=\"id\", how=\"left\")\n",
    "    \n",
    "    row = result_df.collect()[0]\n",
    "    assert row[\"photo_count\"] == 2, \"Photo count aggregation failed\"\n",
    "    print(\"Test 4 Passed: Content enrichment join is working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73bd193-86ce-4152-8fee-f14f4f62efc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_transaction_quarantine()\n",
    "test_geography_deduplication()\n",
    "test_catalog_key_standardization()\n",
    "test_car_content_enrichment()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Silver_Transformations_Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
